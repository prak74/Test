import numpy as np

# X is input as dataframe(m,n)
# n, m are the usual notations
# Y is input as dataframe(m,1)
# all operations using X as matrix(n,m) & Y as matrix(1,m)
# using ReLU activation for hidden layers
# sigmoid activation for output layer

def sigmoid(Z):
	A = 1/(1+np.exp(-Z))
	return A, Z

def relu(Z):
	A = np.maximum(0,Z)
	return A, Z

def sigmoid_backward(dA, Z):
	A, Z = sigmoid(Z)
	dZ = dA*A*(1-A)
	return dZ

def relu_backward(dA, Z):
	dZ = np.array(dA, copy=True)
	dZ[Z <= 0] = 0
	return dZ

def initialize_parameters(layers_dims):
	np.random.seed(1)
	parameters = {}
	L = len(layers_dims)
	for l in range(1,L):
		parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2/layers_dims[l-1]) # standardising the random initialization
		parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
	return parameters

def linear_forward(A_prev, W, b):
	Z = np.dot(W,A_prev) + b
	cache = (A_prev, W, b)
	return Z, cache

def linear_activation_forward(A_prev, W, b, activation):

	if activation == "sigmoid":
		Z, linear_cache = linear_forward(A_prev, W, b)
		A, activation_cache = sigmoid(Z)     
	elif activation == "relu":
		Z, linear_cache = linear_forward(A_prev, W, b)
		A, activation_cache = relu(Z)
	cache = (linear_cache, activation_cache)
	return A, cache

def forward_propagation(X, parameters):

	caches = []
	A = X
	L = len(parameters) // 2    
	for l in range(1, L):
		A_prev = A 
		A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], "relu")
		caches.append(cache)
	AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], "sigmoid")
	caches.append(cache)       
	return AL, caches

def compute_cost(AL, Y, parameters, lambd):
	m = Y.shape[1]
	cost = - np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))/m
	r_cost = 0
	L = len(parameters)//2 
	for l in range(1,L+1):
		r_cost = r_cost + np.sum(parameters["W"+str(l)]**2)
	cost = cost + lambd*r_cost/(2*m)
	return cost

def linear_backward(dZ, cache, lambd):

	A_prev, W, b = cache
	m = A_prev.shape[1]
	dW = np.dot(dZ,A_prev.T)/m + lambd*W/m
	db = np.sum(dZ,axis=1,keepdims=True)/m
	dA_prev = np.dot(W.T,dZ)
	return dA_prev, dW, db

def linear_activation_backward(dA, cache, activation, lambd):

	linear_cache, activation_cache = cache
	if activation == "relu":
		dZ = relu_backward(dA, activation_cache)
		dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)
	elif activation == "sigmoid":
		dZ = sigmoid_backward(dA, activation_cache)
		dA_prev, dW, db = linear_backward(dZ, linear_cache, lambd)
	return dA_prev, dW, db

def backward_propagation(AL, Y, caches, lambd):

	grads = {}
	L = len(caches)
	m = AL.shape[1]
	Y = Y.reshape(AL.shape)    
	dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
	current_cache = caches[L-1]
	grads["dA" + str(L-1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, "sigmoid", lambd)
	for l in reversed(range(L-1)):
		current_cache = caches[l]
		dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(l + 1)], current_cache, "relu", lambd)
		grads["dA" + str(l)] = dA_prev_temp
		grads["dW" + str(l + 1)] = dW_temp
		grads["db" + str(l + 1)] = db_temp
	return grads

def update_parameters(parameters, grads, learning_rate):
	L = len(parameters) // 2
	for l in range(L):
		parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate*grads["dW" + str(l+1)]
		parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate*grads["db" + str(l+1)]
	return parameters

def optimize(X, Y, layers_dims, num_iter, learning_rate, lambd):
	costs=[]
	parameters = initialize_parameters(layers_dims)
	for i in range(0, num_iter):
		AL, caches = forward_propagation(X, parameters)
		cost = compute_cost(AL, Y, parameters, lambd)
		grads = backward_propagation(AL, Y, caches, lambd)
		parameters = update_parameters(parameters, grads, learning_rate)
	return parameters

def train(X_train, Y_train, layers_dims=[12,8,4], num_iter=50000, learning_rate=0.01, lambd=0):  # layers_dims = list of no. of hidden units of each hidden layer
	X_train = X_train.to_numpy().T
	Y_train = Y_train.to_numpy().reshape((1,X_train.shape[1]))
	layers_dims = [X_train.shape[0]] + layers_dims + [Y_train.shape[0]]
	parameters = optimize(X_train, Y_train, layers_dims, num_iter, learning_rate, lambd)
	return parameters

def predict(X_test, parameters):
	X_test = X_test.to_numpy().T
	AL, caches = forward_propagation(X_test, parameters)
	Y_predict = (AL > 0.5).astype(int)
	return Y_predict.T

def accuracy(Y_predict, Y_test):
	Y_test = Y_test.to_numpy().reshape((Y_test.shape[0],1))
	c = 0
	m = Y_test.shape[0]
	for i in range(Y_test.shape[0]):
		if Y_test[i,0]==Y_predict[i,0]:
			c+=1
	return c/m